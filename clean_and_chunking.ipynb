{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da477add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanphat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanphat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanphat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tanphat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\tanphat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tanphat\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.2.0 nltk-3.9.1 regex-2024.11.6\n"
     ]
    }
   ],
   "source": [
    "pip install nltk tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "491e2362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TANPHAT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5792321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_for_chunking(text):\n",
    "    # 1. Thay th·∫ø k√Ω t·ª± Unicode l·ªói\n",
    "    replacements = {\n",
    "        \"ÔÄΩ\": \"=\", \"ÔÄ≠\": \"-\", \"ÔÇ¥\": \"√ó\", \"ÔÇ∏\": \"√∑\", \"ÔÉ•\": \"‚àë\",\n",
    "        \"ÔÇ±\": \"¬±\", \"ÔÇÆ\": \"‚Üí\", \"ÔÉ†\": \"‚Üí\", \"¬†\": \" \", \"ÔÇ£\": \"<=\", \"ÔÇ≥\": \">=\"\n",
    "    }\n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "\n",
    "    # 2. T√°ch d√≤ng v√† x·ª≠ l√Ω d√≤ng\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # 3. Lo·∫°i b·ªè d√≤ng r·ªùi r·∫°c in HOA\n",
    "        if len(line.split()) < 5 and line.isupper():\n",
    "            continue\n",
    "\n",
    "        # 4. Lo·∫°i b·ªè d√≤ng ch·ª©a link ho·∫∑c bi·ªÉu m·∫´u\n",
    "        if re.search(r\"http[s]?://|facebook\\.com|Email:|M·∫≠t kh·∫©u|ƒêƒÉng nh·∫≠p|CCCD|H·ªç t√™n\", line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # 5. Chu·∫©n h√≥a d·∫•u ch·∫•m c√¢u cu·ªëi c√¢u (n·∫øu thi·∫øu v√† kh√¥ng ph·∫£i danh s√°ch ch·∫•m ƒë·∫ßu d√≤ng)\n",
    "        if not re.search(r\"[.!?‚Ä¶:]$\", line) and not line.startswith(\"-\") and not line.startswith(\"+\"):\n",
    "            line += \".\"\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    # 6. G·ªôp l·∫°i th√†nh vƒÉn b·∫£n s·∫°ch\n",
    "    return \" \".join(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf5710ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned: admission_text/De an TS2022 HUS.txt\n",
      "‚úÖ Cleaned: admission_text/index.php_Home_viewnewsVNU_1185.txt\n",
      "‚úÖ Cleaned: admission_text/index.php_Home_viewpage_143.txt\n",
      "‚úÖ Cleaned: admission_text/index.php_Home_viewpage_57.txt\n",
      "‚úÖ Cleaned: admission_text/XHNV_De an tuyen sinh trinh do dai hoc nam 2022 (kem CV1688).txt\n",
      "‚úÖ Cleaned: programs_text/home_C1885.txt\n",
      "‚úÖ Cleaned: programs_text/home_C1963.txt\n",
      "‚úÖ Cleaned: programs_text/home_C1965.txt\n",
      "‚úÖ Cleaned: programs_text/home_C2019.txt\n",
      "‚úÖ Cleaned: programs_text/home_C2020.txt\n",
      "‚úÖ Cleaned: programs_text/home_C2452.txt\n",
      "‚úÖ Cleaned: programs_text/home_C2455.txt\n",
      "‚úÖ Cleaned: programs_text/home_C2456.txt\n",
      "‚úÖ Cleaned: regulations_text/3626_21.10.2022quy-che-dao-tao-dai-hoc-tai-dhqghn-ap-dung-tu-khoa-qh2022.txt\n",
      "‚úÖ Cleaned: regulations_text/Quy-ch·∫ø-ƒêTƒêH-3626.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1700.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1701.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1702.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1703.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1704.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1705.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1706.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1707.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1708.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1709.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1710.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1711.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1885.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1916.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1917.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1918.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1919.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1963.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C1965.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2015.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2019.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2020.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2038.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2039.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2040.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2042.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2451.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2452.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2455.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2456.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2575.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2731.txt\n",
      "‚úÖ Cleaned: vnu_text/home_C2758.txt\n",
      "‚úÖ Cleaned: wiki/wiki_text.txt\n",
      "ƒê√£ t·∫°o 49 file trong: data_clean_verified\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = \"data_clean\"\n",
    "OUTPUT_DIR = \"data_clean_verified\"\n",
    "\n",
    "file_count = 0\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for folder in os.listdir(INPUT_DIR):\n",
    "    input_subdir = os.path.join(INPUT_DIR, folder)\n",
    "    output_subdir = os.path.join(OUTPUT_DIR, folder)\n",
    "    os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "    if not os.path.isdir(input_subdir):\n",
    "        continue\n",
    "\n",
    "    for fname in os.listdir(input_subdir):\n",
    "        if not fname.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(input_subdir, fname)\n",
    "        output_path = os.path.join(output_subdir, fname)\n",
    "\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "\n",
    "        cleaned_text = clean_text_for_chunking(raw_text)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned_text)\n",
    "\n",
    "        file_count += 1\n",
    "        print(f\"‚úÖ Cleaned: {folder}/{fname}\")\n",
    "\n",
    "print(f\"ƒê√£ t·∫°o {file_count} file trong: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a2a3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CHUNK CONFIG ==========\n",
    "MIN_CHUNK_LEN = 200\n",
    "MAX_CHUNK_LEN = 500\n",
    "INPUT_DIR = \"data_clean_verified\"\n",
    "OUTPUT_FILE = \"chunks/chunks.jsonl\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b688a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫£i d·ªØ li·ªáu NLTK (c·∫ßn cho ph√¢n t√°ch c√¢u)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c4b4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_by_char(text, min_len=200, max_len=500):\n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current = ''\n",
    "    for sentence in sentences:\n",
    "        if len(current) + len(sentence) <= max_len:\n",
    "            current += ' ' + sentence\n",
    "        else:\n",
    "            if len(current.strip()) >= min_len:\n",
    "                chunks.append(current.strip())\n",
    "            current = sentence\n",
    "    if len(current.strip()) >= min_len:\n",
    "        chunks.append(current.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4ada1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X·ª¨ L√ù M·ªòT FILE \n",
    "def process_single_file(file_path, category):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    chunks = chunk_text_by_char(text)\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    result = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        result.append({\n",
    "            \"source_file\": file_name,\n",
    "            \"category\": category,\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk\n",
    "        })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f51bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  X·ª¨ L√ù TO√ÄN B·ªò TH∆Ø M·ª§C \n",
    "def process_all_files(input_root, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "        for category in os.listdir(input_root):\n",
    "            folder_path = os.path.join(input_root, category)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "\n",
    "            for fname in tqdm(os.listdir(folder_path), desc=f\"üìÅ {category}\"):\n",
    "                if not fname.endswith(\".txt\"):\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(folder_path, fname)\n",
    "                chunks = process_single_file(file_path, category)\n",
    "                for chunk in chunks:\n",
    "                    out.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nƒê√£ t·∫°o chunk t·ª´ {input_root} v√†o: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bff5b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìÅ admission_text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 287.40it/s]\n",
      "üìÅ programs_text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 515.81it/s]\n",
      "üìÅ regulations_text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 307.29it/s]\n",
      "üìÅ vnu_text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:00<00:00, 851.01it/s]\n",
      "üìÅ wiki: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1000.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ƒê√£ t·∫°o chunk t·ª´ data_clean_verified v√†o: chunks/chunks.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_all_files(INPUT_DIR, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0172ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_chunks_stats(chunk_file_path, min_len=200, max_len=500):\n",
    "    from collections import Counter\n",
    "    import json\n",
    "\n",
    "    lengths = []\n",
    "    short, long, valid = 0, 0, 0\n",
    "    with open(chunk_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            entry = json.loads(line)\n",
    "            text_len = len(entry.get(\"text\", \"\"))\n",
    "            lengths.append(text_len)\n",
    "            if text_len < min_len:\n",
    "                short += 1\n",
    "            elif text_len > max_len:\n",
    "                long += 1\n",
    "            else:\n",
    "                valid += 1\n",
    "\n",
    "    print(f\"T·ªïng chunk: {len(lengths)}\")\n",
    "    print(f\"H·ª£p l·ªá ({min_len}‚Äì{max_len}): {valid}\")\n",
    "    print(f\"Qu√° ng·∫Øn (<{min_len}): {short}\")\n",
    "    print(f\"Qu√° d√†i (>{max_len}): {long}\")\n",
    "    print(f\"Trung b√¨nh: {sum(lengths) // len(lengths)} k√Ω t·ª±\")\n",
    "    print(f\"Ph√¢n b·ªë ƒë·ªô d√†i:\")\n",
    "    for key in sorted(Counter([l//100*100 for l in lengths]).items()):\n",
    "        print(f\"  {key[0]}-{key[0]+99}: {key[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d4cc9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng chunk: 1569\n",
      "H·ª£p l·ªá (200‚Äì500): 1448\n",
      "Qu√° ng·∫Øn (<200): 0\n",
      "Qu√° d√†i (>500): 121\n",
      "Trung b√¨nh: 464 k√Ω t·ª±\n",
      "Ph√¢n b·ªë ƒë·ªô d√†i:\n",
      "  200-299: 65\n",
      "  300-399: 111\n",
      "  400-499: 1236\n",
      "  500-599: 140\n",
      "  600-699: 7\n",
      "  700-799: 2\n",
      "  800-899: 1\n",
      "  900-999: 1\n",
      "  1000-1099: 3\n",
      "  1300-1399: 2\n",
      "  1400-1499: 1\n"
     ]
    }
   ],
   "source": [
    "verify_chunks_stats(\"chunks/chunks.jsonl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
